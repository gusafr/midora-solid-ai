# Implementing AI Agents: Practical Guide

**A step-by-step playbook for defining, deploying, and governing AI agents in your organization**

---

## Overview

This playbook provides practical guidance for implementing AI agents within the SOLID.AI framework. You'll learn how to:

1. **Define agent roles** using the 4-level hierarchy (Assistant â†’ Consultant â†’ Specialist â†’ Director)
2. **Set appropriate autonomy levels** (Supervised â†’ Co-pilot â†’ Automated â†’ Governing)
3. **Choose the right technology stack** for your use case
4. **Deploy safely** with human oversight and governance
5. **Measure success** and iterate

**Time to First Agent:** 1-2 weeks for pilot, 2-3 months for production-ready agents

---

## Part 1: The 4-Level Agent Hierarchy

### Quick Reference

| Level | Role Types | Autonomy | Scope | Human Oversight |
|-------|-----------|----------|-------|-----------------|
| **Low** | Assistant, Analyst | Supervised | Single task, narrow domain | Review every action |
| **Intermediate** | Consultant, Coordinator | Co-pilot | Multi-step workflows, broader context | Review decisions, not tasks |
| **High** | Specialist, Manager | Automated | Domain expertise, strategic execution | Review outcomes, exceptions only |
| **Executive** | Director, VP | Governing | Cross-domain strategy, organizational decisions | Set direction, approve major changes |

### Level 1: Low-Level Agents (Start Here)

**When to Use:**
- Repetitive, high-volume tasks (data entry, email responses, reports)
- Well-defined inputs/outputs (invoice â†’ structured data)
- Low risk if error occurs (can be corrected easily)

**Examples:**
- **InvoiceProcessor-Agent**: Extract data from PDFs â†’ Accountant reviews before payment
- **LeadQualifier-Agent**: Score inbound leads â†’ SDR reviews before outreach
- **ChatbotSupport-Agent**: Answer FAQs â†’ Escalate complex questions to humans

**Technology Stack:**
- **Simple automation**: Zapier, Make.com, n8n
- **AI APIs**: OpenAI GPT-4, Anthropic Claude, Google Gemini
- **Document processing**: Docparser, OCR APIs, LangChain
- **Integration**: REST APIs, webhooks

**Autonomy Level:** **Supervised** (human reviews every action)

**Template Definition:**

```yaml
agent:
  identity:
    name: "LeadQualifier-Agent"
    level: "Low (Assistant)"
    role: "Score and enrich inbound leads"
    
  capabilities:
    - task: "Score lead quality"
      input: "Form submission (name, email, company, role)"
      output: "Lead score 0-100 + enriched data (company size, industry, tech stack)"
      performance: "Process in <10 seconds, 90% accuracy"
      
  technology:
    ai_model: "OpenAI GPT-4o-mini"
    integration: "Zapier webhook â†’ Clearbit API â†’ Salesforce"
    cost: "$0.05/lead"
    
  guardrails:
    prohibited:
      - "Do not auto-assign leads without SDR review"
      - "Do not contact leads directly"
    boundaries:
      - "Escalate scores <30 or >90 to sales manager"
      
  human_oversight:
    autonomy_level: "supervised"
    review: "SDR reviews all scored leads daily"
    escalation: "Sales Manager reviews edge cases weekly"
    
  success_metrics:
    efficiency: "200 leads/day (vs. 20 manual)"
    quality: "90% score accuracy"
    value: "Save 4 SDR hours/day"
```

---

### Level 2: Intermediate Agents (Once Low-Level Proven)

**When to Use:**
- Multi-step workflows requiring judgment (not just execution)
- Coordination across systems/people
- Moderate risk (mistakes noticeable but recoverable)

**Examples:**
- **SprintPlanner-Agent**: Analyze backlog â†’ Suggest sprint composition â†’ Human approves
- **ChurnPredictor-Agent**: Identify at-risk customers â†’ Draft outreach email â†’ CSM reviews and sends
- **ExpenseApprover-Agent**: Review expense reports â†’ Auto-approve <$500, escalate >$500

**Technology Stack:**
- **Workflow orchestration**: Temporal.io, Apache Airflow, Prefect
- **AI reasoning**: OpenAI GPT-4, Claude 3.5 Sonnet, Llama 3
- **Data integration**: APIs, ETL pipelines, data warehouses
- **Decision tracking**: Audit logs, observability tools

**Autonomy Level:** **Co-pilot** (AI suggests, human decides on critical actions)

**Template Definition:**

```yaml
agent:
  identity:
    name: "ChurnPredictor-Agent"
    level: "Intermediate (Analyst)"
    role: "Identify at-risk customers, suggest retention actions"
    
  capabilities:
    - task: "Predict churn risk"
      input: "Usage data, support tickets, payment history, engagement metrics"
      output: "Churn score 0-100 + reasoning + suggested action"
      performance: "Predict 70% of churn 60+ days early"
      
    - task: "Draft retention email"
      input: "Customer profile, churn reason hypothesis"
      output: "Personalized email (tone-matched to customer relationship)"
      performance: "80% of drafts sent with minor edits"
      
  technology:
    ai_model: "Claude 3.5 Sonnet (reasoning), GPT-4o (email generation)"
    data_source: "Snowflake data warehouse, Segment events, Zendesk API"
    orchestration: "Temporal workflow (daily batch + real-time alerts)"
    cost: "$0.50/customer/month"
    
  guardrails:
    prohibited:
      - "Do not contact customers without CSM approval"
      - "Do not offer discounts without pricing authority"
    boundaries:
      - "Escalate VIP customers (>$100K ARR) immediately"
      - "Escalate if churn reason unclear (confidence <60%)"
      
  human_oversight:
    autonomy_level: "co-pilot"
    review: "CSM reviews daily churn report, approves outreach"
    escalation: "VP Customer Success reviews weekly for model accuracy"
    feedback_loop: "CSM marks 'good catch' or 'false alarm' â†’ Retrains model monthly"
    
  success_metrics:
    business_value: "Reduce churn 6% â†’ 3% (save $360K ARR/year)"
    accuracy: "70% churn prediction accuracy at 60-day lead time"
    efficiency: "CSM manages 200 customers (vs. 50 reactive)"
```

---

### Level 3: High-Level Agents (For Mature AI-Native Orgs)

**When to Use:**
- Domain expertise at scale (legal review, code architecture, financial modeling)
- Strategic execution (not just tactical tasks)
- High value, high complexity (requires deep knowledge)

**Examples:**
- **CodeReviewer-Agent**: Review PRs for security, performance, best practices â†’ Auto-approve if passing
- **ContractAnalyzer-Agent**: Review vendor contracts â†’ Flag risks â†’ Legal team reviews exceptions
- **BudgetOptimizer-Agent**: Reallocate budget across departments â†’ CFO approves quarterly

**Technology Stack:**
- **Specialized AI**: Fine-tuned LLMs, domain-specific models
- **RAG (Retrieval-Augmented Generation)**: Vector databases (Pinecone, Weaviate) + LLMs
- **Autonomous frameworks**: LangChain Agents, AutoGPT, BabyAGI
- **Enterprise platforms**: Azure OpenAI, AWS Bedrock, Google Vertex AI

**Autonomy Level:** **Automated** (acts independently within boundaries, human reviews outcomes)

**Template Definition:**

```yaml
agent:
  identity:
    name: "CodeReviewer-Agent"
    level: "High (Specialist)"
    role: "Automated code review for security, performance, best practices"
    
  capabilities:
    - task: "Security review"
      input: "Pull request (code diff, dependencies)"
      output: "Security report (vulnerabilities, OWASP top 10, secrets detection)"
      performance: "Detect 95% of known vulnerabilities"
      
    - task: "Performance analysis"
      input: "Code diff, performance benchmarks"
      output: "Performance impact estimate (latency, memory, CPU)"
      performance: "Flag regressions >10%"
      
    - task: "Best practices enforcement"
      input: "Code diff, style guide, architectural patterns"
      output: "Suggestions (naming, structure, readability)"
      performance: "95% alignment with team standards"
      
  technology:
    ai_model: "GPT-4 (code understanding) + CodeQL (static analysis)"
    knowledge_base: "RAG: 1000+ past PRs, internal docs, OWASP database"
    integration: "GitHub Actions, pre-commit hooks"
    cost: "$2/PR"
    
  guardrails:
    prohibited:
      - "Do not block critical hotfixes (override available)"
      - "Do not auto-merge without passing tests"
    boundaries:
      - "Escalate architectural changes to Staff Engineer"
      - "Escalate external dependency changes to Security team"
      
  human_oversight:
    autonomy_level: "automated (within safety bounds)"
    review: "Senior engineer reviews flagged issues, overrides if needed"
    escalation: "Security team reviews high/critical vulnerabilities"
    feedback_loop: "Engineers mark false positives â†’ Retrains model weekly"
    
  success_metrics:
    quality: "Reduce production bugs 30%"
    speed: "Code review time: 2 hours â†’ 15 minutes"
    coverage: "Review 100% of PRs (vs. 60% manual)"
```

---

### Level 4: Executive Agents (Future State)

**When to Use:**
- Cross-domain strategic decisions (portfolio allocation, M&A analysis)
- Organizational governance (policy enforcement, compliance monitoring)
- High-stakes, high-complexity (requires executive judgment)

**Examples:**
- **PortfolioOptimizer-Agent**: Model investment scenarios â†’ Recommend budget allocation â†’ CEO approves
- **ComplianceMonitor-Agent**: Track regulatory changes â†’ Update policies â†’ Legal team reviews
- **M&A-Analyzer-Agent**: Evaluate acquisition targets â†’ Due diligence report â†’ Board reviews

**Technology Stack:**
- **Advanced AI**: Multi-agent systems, reasoning models (o1, o3)
- **Decision support**: Scenario modeling, Monte Carlo simulations
- **Governance platforms**: Enterprise AI orchestration, audit trails

**Autonomy Level:** **Governing** (sets strategic direction, humans approve major changes)

**Note:** Most organizations should focus on Low/Intermediate agents first. Executive agents require mature AI governance.

---

## Part 2: Technology Selection Guide

### Decision Matrix: Choosing the Right Stack

| Use Case | Complexity | Recommended Stack | Cost | Time to Deploy |
|----------|-----------|-------------------|------|----------------|
| **Simple automation** (email, data entry) | Low | Zapier + OpenAI API | $50-200/mo | 1-2 days |
| **Workflow orchestration** (multi-step) | Medium | n8n + Claude API | $200-500/mo | 1-2 weeks |
| **Custom agents** (domain-specific) | High | LangChain + RAG + Fine-tuned LLM | $1K-5K/mo | 4-8 weeks |
| **Enterprise platform** (governance, scale) | Very High | Azure OpenAI + Custom framework | $10K-50K/mo | 3-6 months |

---

### Technology Stack Components

#### 1. **AI Models (The Brain)**

**General-Purpose LLMs:**
- **OpenAI GPT-4o**: Best for reasoning, writing, general tasks ($5/1M tokens)
- **Anthropic Claude 3.5 Sonnet**: Best for long context, analysis ($3/1M tokens)
- **Google Gemini 1.5 Pro**: Best for multimodal (text, image, video) ($1.25/1M tokens)
- **Meta Llama 3**: Best for self-hosted, privacy-critical (free, infrastructure cost)

**Specialized Models:**
- **Code**: GitHub Copilot, Codex, CodeLlama
- **Document extraction**: AWS Textract, Google Document AI
- **Embeddings**: OpenAI text-embedding-3, Cohere, Voyage AI

**When to Use What:**
- **Prototyping**: OpenAI GPT-4o-mini (cheap, fast)
- **Production**: Claude 3.5 Sonnet (reliable, safe)
- **High-volume**: Llama 3 (self-hosted, cost-efficient at scale)
- **Regulated industries**: Azure OpenAI (enterprise SLA, compliance)

---

#### 2. **Orchestration (The Workflow Engine)**

**No-Code/Low-Code:**
- **Zapier**: Easiest, 5000+ integrations, $20-100/mo
- **Make.com**: Visual workflows, cheaper than Zapier, $9-30/mo
- **n8n**: Self-hosted, free, highly customizable

**Code-Based:**
- **LangChain**: Python/JS framework for AI agents, free (OSS)
- **Temporal.io**: Durable workflows, great for complex multi-step, free tier + paid
- **Apache Airflow**: Data pipeline orchestration, free (OSS)

**When to Use What:**
- **Non-technical teams**: Zapier, Make.com
- **Developers**: LangChain, Temporal
- **Data engineers**: Airflow

---

#### 3. **Knowledge Management (The Memory)**

**Vector Databases (for RAG):**
- **Pinecone**: Managed, easiest to use, $70/mo+
- **Weaviate**: Open-source, self-hosted, free
- **Qdrant**: Fast, modern, free tier + paid
- **PostgreSQL + pgvector**: Simplest for existing Postgres users, free

**When to Use RAG:**
- Agent needs company-specific knowledge (docs, past tickets, codebases)
- Agent must cite sources (compliance, legal)
- Agent should improve over time (add new knowledge without retraining)

**Architecture:**
```
User Query â†’ Embed with LLM â†’ Search Vector DB â†’ Retrieve Top 5 Docs â†’ 
Augment LLM Prompt â†’ Generate Response
```

---

#### 4. **Integration Layer (The Connectors)**

**APIs & Webhooks:**
- **REST APIs**: Standard for most SaaS (Salesforce, Jira, Stripe)
- **Webhooks**: Real-time event triggers (form submission, payment received)
- **GraphQL**: Flexible queries (GitHub, Shopify)

**iPaaS (Integration Platform as a Service):**
- **Zapier**: 5000+ pre-built connectors
- **Workato**: Enterprise-grade, complex workflows
- **Tray.io**: Visual, powerful for large teams

---

#### 5. **Observability (The Control Tower)**

**Monitoring AI Agents:**
- **LangSmith** (LangChain): Trace agent decisions, debug failures
- **Helicone**: AI observability for OpenAI/Claude/etc.
- **Datadog, New Relic**: General application monitoring + AI traces

**What to Monitor:**
- **Latency**: Response time (target: <5 seconds for interactive agents)
- **Cost**: Token usage, API calls (track per agent)
- **Accuracy**: Human feedback (thumbs up/down), error rate
- **Safety**: Guardrail violations, escalations to humans

---

## Part 3: Implementation Roadmap

### Phase 1: Pilot (Weeks 1-2)

**Goal:** Deploy 1 low-level agent, prove value

**Steps:**

1. **Choose Use Case** (1 hour)
   - Pick high-volume, low-risk task (e.g., lead qualification, invoice processing)
   - Identify success metric (e.g., "Save 4 hours/day")

2. **Define Agent** (2 hours)
   - Use template above
   - Document inputs, outputs, guardrails
   - Set autonomy level (start with "supervised")

3. **Select Technology** (1 hour)
   - Start simple: Zapier + OpenAI API
   - Example: Form submission â†’ GPT-4 scores lead â†’ Slack notification

4. **Build & Test** (3 days)
   - Set up workflow in Zapier
   - Test with 10 sample inputs
   - Measure accuracy (target: >80%)

5. **Deploy with Human Review** (1 week)
   - Run in parallel with manual process
   - Human reviews every AI action
   - Collect feedback: "Was this useful? Accurate?"

6. **Measure & Iterate** (ongoing)
   - Track time saved, accuracy, user satisfaction
   - Adjust prompts, guardrails based on feedback

**Success Criteria:**
- âœ… Agent processes >80% of inputs correctly
- âœ… Saves >2 hours/day
- âœ… Team trusts agent enough to reduce review frequency

---

### Phase 2: Scale (Weeks 3-8)

**Goal:** Deploy 3-5 agents across teams, establish governance

**Steps:**

1. **Identify High-Impact Use Cases** (1 week)
   - Survey teams: "What repetitive tasks consume your time?"
   - Prioritize by: Volume Ã— Time per task Ã— Error risk
   - Top candidates: Data entry, email drafting, report generation, lead scoring

2. **Deploy Intermediate Agents** (4 weeks)
   - Build 3-5 agents (one per team: Sales, CS, Finance, etc.)
   - Use orchestration (n8n or LangChain for multi-step workflows)
   - Implement RAG if domain knowledge needed

3. **Establish Governance** (ongoing)
   - Create AI Agent Registry (track all agents, owners, metrics)
   - Define review cadence (weekly for new agents, monthly for mature)
   - Set escalation paths (when does agent hand off to human?)

4. **Train Teams** (2 weeks)
   - Workshops: "How to work with your AI agent teammate"
   - Document SOPs: "When to trust agent, when to override"
   - Feedback loops: "How to improve your agent"

**Success Criteria:**
- âœ… 5 agents deployed, each saving >5 hours/week
- âœ… <10% false positive rate (errors that humans must fix)
- âœ… >80% employee satisfaction ("AI makes my job easier")

---

### Phase 3: Maturity (Months 3-6)

**Goal:** High-level agents, autonomous execution, continuous improvement

**Steps:**

1. **Deploy High-Level Agents** (8 weeks)
   - Specialized agents (CodeReviewer, ContractAnalyzer)
   - Autonomous within guardrails (auto-approve if criteria met)
   - Requires: Fine-tuning, RAG, domain expertise

2. **Implement Continuous Learning** (ongoing)
   - Capture human feedback (accept/reject AI suggestions)
   - Retrain models monthly with new data
   - A/B test prompt improvements

3. **Scale Across Organization** (ongoing)
   - 20+ agents covering all major workflows
   - Self-service agent deployment (teams create their own)
   - AI Agent Marketplace (share successful agents across teams)

**Success Criteria:**
- âœ… 50%+ of repetitive work automated
- âœ… Agent accuracy >90% (minimal human intervention)
- âœ… Teams proactively identify new agent opportunities

---

## Part 4: Governance & Safety

### Autonomy Ladder: Progressive Trust

Start agents at **Supervised**, promote based on performance:

| Autonomy Level | Description | Promotion Criteria |
|----------------|-------------|-------------------|
| **Supervised** | Human reviews every action | - |
| **Co-pilot** | AI suggests, human approves critical actions | >90% accuracy for 2 weeks |
| **Automated** | AI acts within guardrails, human reviews outcomes | >95% accuracy for 1 month, zero safety violations |
| **Governing** | AI makes strategic decisions, human approves major changes | >98% accuracy for 3 months, proven ROI |

**Never skip levels.** Always start Supervised, earn autonomy.

---

### Guardrails Framework

Every agent needs 3 types of guardrails:

#### 1. **Prohibited Actions** (Hard Limits)
- "Do not send emails without human review"
- "Do not approve payments >$5K"
- "Do not modify production database"

#### 2. **Boundary Conditions** (Escalation Triggers)
- "Escalate if confidence <70%"
- "Escalate VIP customers to manager"
- "Escalate if legal/compliance keywords detected"

#### 3. **Performance Thresholds** (Quality Gates)
- "If error rate >10%, pause and alert human"
- "If cost >$100/day, notify owner"
- "If latency >10 seconds, degrade gracefully"

**Enforcement:**
- Technical: API rate limits, budget caps, approval workflows
- Procedural: Weekly reviews, audit logs, incident response

---

### Human Oversight Models

| Model | When to Use | Example |
|-------|-------------|---------|
| **Human-in-the-loop** | High-risk, low-volume | Contract approval, hiring decisions |
| **Human-on-the-loop** | Medium-risk, high-volume | Code review, expense approval |
| **Human-out-of-the-loop** | Low-risk, very high-volume | Data entry, FAQ chatbot |

**Best Practice:** Start with human-in-the-loop, evolve to human-on-the-loop as trust builds.

---

## Part 5: Measuring Success

### Agent Performance Scorecard

Track these metrics for every agent:

#### Business Value
- **Time saved**: Hours/week automated (compare before/after)
- **Cost reduction**: Labor cost saved - AI cost
- **Revenue impact**: Deals closed faster, churn reduced, upsells generated

#### Quality
- **Accuracy**: % of agent actions correct (human-verified)
- **Precision**: % of positive predictions that are correct (minimize false positives)
- **Recall**: % of relevant cases detected (minimize false negatives)

#### User Satisfaction
- **Ease of use**: "How easy is it to work with this agent?" (1-5 scale)
- **Trust**: "How often do you override the agent?" (lower = better)
- **Impact**: "Does this agent make your job better?" (% yes)

#### Safety & Compliance
- **Guardrail violations**: Count of prohibited actions attempted
- **Escalations**: % of cases requiring human intervention
- **Audit trail**: 100% of decisions logged and explainable

---

### Sample Dashboard

```
AGENT: LeadQualifier-Agent
Status: ðŸŸ¢ Healthy

Business Value:
  Time saved: 20 hours/week (5 SDRs Ã— 4 hours each)
  Cost: $150/month (API + Zapier)
  ROI: 133x ($20K labor saved / $150 cost)

Quality:
  Accuracy: 92% (46/50 leads scored correctly this week)
  False positives: 6% (3 low-quality leads marked high)
  False negatives: 2% (1 high-quality lead missed)

User Satisfaction:
  Ease of use: 4.8/5
  Override rate: 8% (SDRs manually adjust 4/50 scores)
  Impact: 100% of SDRs say "makes job easier"

Safety:
  Guardrail violations: 0
  Escalations: 12% (edge cases sent to manager)
  Audit trail: 100% logged
```

**Action:** If accuracy <85% or satisfaction <3.5, pause and improve.

---

## Part 6: Common Pitfalls & How to Avoid Them

| Pitfall | Impact | Solution |
|---------|--------|----------|
| **Starting too complex** (trying to build AGI) | Project fails, team discouraged | Start with low-level agents (data entry, FAQs) |
| **No human oversight** (full autonomy from day 1) | Errors accumulate, trust lost | Always start Supervised, earn autonomy gradually |
| **Ignoring edge cases** (agent trained on happy path) | Fails in real world (10% edge cases = 90% of problems) | Test with messy, real-world data, not just clean samples |
| **Over-automating** (removing humans from creative work) | Employee resentment, quality drops | Focus AI on busywork, keep humans in high-value roles |
| **No feedback loop** (deploy and forget) | Agent doesn't improve, becomes obsolete | Weekly reviews, monthly retraining, continuous iteration |
| **Unclear ownership** (who's responsible for agent?) | No one fixes issues, agent drifts | Assign owner (person + team), track in registry |

---

## Part 7: Quick Start Checklist

### Week 1: Choose & Define

- [ ] Identify 1 high-volume, low-risk task to automate
- [ ] Define agent using template (identity, capabilities, guardrails)
- [ ] Set success metrics (time saved, accuracy, satisfaction)
- [ ] Get stakeholder buy-in (team using agent, manager approving budget)

### Week 2: Build & Test

- [ ] Choose technology (start simple: Zapier + OpenAI)
- [ ] Build workflow (inputs â†’ AI â†’ outputs)
- [ ] Test with 10 samples (measure accuracy)
- [ ] Set up monitoring (latency, cost, errors)

### Week 3-4: Deploy & Learn

- [ ] Run in parallel with manual process (human reviews every action)
- [ ] Collect feedback ("Was this helpful? Accurate?")
- [ ] Iterate on prompts, guardrails
- [ ] Measure impact (time saved, accuracy)

### Month 2: Scale

- [ ] Deploy 3-5 agents across teams
- [ ] Establish governance (registry, reviews, escalations)
- [ ] Train teams on working with AI agents
- [ ] Share wins company-wide

### Month 3+: Optimize

- [ ] Promote high-performing agents to higher autonomy
- [ ] Retire underperforming agents
- [ ] Continuous improvement (retrain monthly)
- [ ] Expand to more complex use cases

---

## Conclusion: Start Small, Think Big

**The path to AI-native organization:**

1. **Week 1**: Deploy your first agent (data entry, lead scoring, FAQ bot)
2. **Month 1**: Prove value (save 10+ hours/week)
3. **Month 3**: Scale to 5+ agents (save 50+ hours/week)
4. **Month 6**: Agents handle 50% of repetitive work
5. **Year 1**: AI-native cultureâ€”humans focus on creativity, strategy, relationships

**Remember:**
- âœ… Start with low-level, supervised agents
- âœ… Earn autonomy through performance
- âœ… Measure relentlessly (value, quality, safety)
- âœ… Keep humans in the loop (AI assists, humans decide)
- âœ… Iterate continuously (feedback â†’ retrain â†’ improve)

The goal isn't to replace humansâ€”it's to **free them to do what they do best**.

---

## Appendix: Agent Definition Templates

### Template 1: Simple Assistant Agent

```yaml
agent:
  identity:
    name: "[AgentName]-Agent"
    level: "Low (Assistant)"
    role: "[One-sentence purpose]"
    
  capabilities:
    - task: "[What it does]"
      input: "[Data it receives]"
      output: "[Data it produces]"
      performance: "[Speed, accuracy targets]"
      
  technology:
    ai_model: "OpenAI GPT-4o-mini"
    integration: "Zapier"
    cost: "$[XX]/month"
    
  guardrails:
    prohibited: ["Do not [action] without [approval]"]
    boundaries: ["Escalate if [condition]"]
    
  human_oversight:
    autonomy_level: "supervised"
    review: "[Who reviews, how often]"
    
  success_metrics:
    value: "Save [X] hours/week"
    quality: "[XX]% accuracy"
```

### Template 2: Advanced Specialist Agent

```yaml
agent:
  identity:
    name: "[AgentName]-Agent"
    level: "High (Specialist)"
    role: "[Strategic purpose]"
    
  capabilities:
    - task: "[Complex task 1]"
      input: "[Multi-source data]"
      output: "[Strategic insights]"
      performance: "[Business outcome]"
      
    - task: "[Complex task 2]"
      input: "[...]"
      output: "[...]"
      performance: "[...]"
      
  technology:
    ai_model: "Claude 3.5 Sonnet + [specialized model]"
    knowledge_base: "RAG: [vector DB] + [data sources]"
    orchestration: "LangChain + Temporal"
    cost: "$[XXX]/month"
    
  guardrails:
    prohibited: ["[Critical limits]"]
    boundaries: ["Escalate if [strategic condition]"]
    thresholds: ["Pause if error rate >[X]%"]
    
  human_oversight:
    autonomy_level: "automated (within bounds)"
    review: "[Senior role] reviews outcomes weekly"
    escalation: "[Executive] approves strategic changes"
    feedback_loop: "[How agent improves over time]"
    
  success_metrics:
    business_value: "[Revenue/cost impact]"
    accuracy: "[XX]% [metric]"
    efficiency: "[Outcome vs. manual baseline]"
```

---

**Next Steps:**
- [Day in the Life](day-in-the-life-ai-native-organization.md) - See agents in action
- [Role Hierarchy](../DOCS/10-role-hierarchy-human-ai.md) - Detailed level definitions
- [Governance](../DOCS/06-governance-ethics.md) - Safety and compliance frameworks

---

**Version:** 1.0  
**Last Updated:** November 2025  
**Framework:** SOLID.AI  
**License:** MIT
