# Data Spine: Structuring and Governing Data in AI-Native Organizations

**A practical guide to building the information backbone that powers SOLID.AI**

---

## Overview

The **Data Spine** is Layer 2 of SOLID.AI‚Äîthe nervous system's backbone that connects everything. Without clean, structured, accessible data, AI agents can't function, automation fails, and decisions are based on guesswork.

**Key Insight:** Most AI failures aren't due to bad algorithms‚Äîthey're due to bad data. Fix the foundation first.

**This Playbook Covers:**
1. Data contracts and schemas
2. Event-driven architecture
3. Data quality and observability
4. Governance and access control
5. Implementation patterns by use case

**Time to Build:** 2-4 weeks for basic spine, 2-3 months for production-ready

---

## Part 1: What is a Data Spine?

### The Problem: Data Chaos

**Traditional Organization (No Data Spine):**
```
Sales data in Salesforce (CRM)
Product usage in Mixpanel (analytics)
Support tickets in Zendesk
Invoices in QuickBooks
HR data in BambooHR
Marketing data in HubSpot

Each system = isolated island
No shared vocabulary ("customer" vs "account" vs "user")
No real-time sync (batch exports, manual CSVs)
No single source of truth (which system is right?)
```

**Result:**
- Finance can't see Sales pipeline
- CS can't see Product usage
- AI agents trained on inconsistent data (60% accuracy)
- Decisions based on outdated spreadsheets

---

### The Solution: Data Spine Architecture

**Data Spine (Unified Backbone):**
```
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ   DATA WAREHOUSE        ‚îÇ
                  ‚îÇ   (Single Source        ‚îÇ
                  ‚îÇ    of Truth)            ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                   ‚îÇ                   ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇSalesforce‚îÇ        ‚îÇMixpanel ‚îÇ        ‚îÇZendesk  ‚îÇ
    ‚îÇ(CRM)     ‚îÇ        ‚îÇ(Analytics)‚îÇ      ‚îÇ(Support)‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                   ‚îÇ                   ‚îÇ
    [Customer Data]     [Usage Data]       [Ticket Data]
         ‚îÇ                   ‚îÇ                   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  DATA CONTRACTS ‚îÇ
                    ‚îÇ  Shared schemas ‚îÇ
                    ‚îÇ  Validation rules‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                   ‚îÇ                   ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇAI Agents‚îÇ        ‚îÇAutomation‚îÇ        ‚îÇAnalytics‚îÇ
    ‚îÇ         ‚îÇ        ‚îÇWorkflows ‚îÇ        ‚îÇDashboards‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Benefits:**
- ‚úÖ **Single source of truth:** One place for customer data
- ‚úÖ **Shared vocabulary:** "Customer" means the same thing everywhere
- ‚úÖ **Real-time sync:** Changes propagate in seconds (not days)
- ‚úÖ **AI-ready data:** Clean, structured, consistent
- ‚úÖ **Cross-functional visibility:** Finance sees Sales, CS sees Product

---

## Part 2: Data Contracts - The Foundation

### What is a Data Contract?

**Definition:** A formal agreement on data structure, quality, and semantics

**Analogy:** API contracts for data
- **API contract:** "POST /users expects {name, email}, returns 201 or 400"
- **Data contract:** "Customer table has {id, name, email, created_at}, email must be valid, id is unique"

---

### Core Data Entities (The Nouns)

Every business has core entities. Define them first:

#### 1. **Customer** (or Account, Client, User)

```yaml
entity: Customer
description: "Organization or individual who pays for our product"
schema:
  id: 
    type: uuid
    required: true
    unique: true
    description: "Unique identifier (never changes)"
  
  name:
    type: string
    required: true
    max_length: 200
    description: "Company name (B2B) or full name (B2C)"
  
  email:
    type: email
    required: true
    unique: true
    validation: "RFC 5322 email format"
    description: "Primary contact email"
  
  status:
    type: enum
    values: [prospect, trial, active, churned, paused]
    required: true
    description: "Lifecycle stage"
  
  created_at:
    type: timestamp
    required: true
    description: "When customer first entered system"
  
  annual_revenue:
    type: decimal
    required: false
    description: "ARR for this customer (USD)"
  
  industry:
    type: string
    required: false
    description: "Customer's industry (e.g., 'Healthcare', 'Finance')"

relationships:
  - has_many: Users (people who work at this customer)
  - has_many: Subscriptions
  - has_many: SupportTickets
  - has_many: UsageEvents

data_quality_rules:
  - "Email must be verified (email_verified = true) before customer marked 'active'"
  - "Status cannot transition 'churned' ‚Üí 'active' without approval"
  - "Annual revenue must be > 0 if status = 'active'"

governance:
  owner: "Sales & CS teams"
  PII: true (contains personal data - GDPR/CCPA applies)
  retention: "7 years after churn (compliance requirement)"
```

#### 2. **User** (Person who uses the product)

```yaml
entity: User
description: "Individual person who logs into the product"
schema:
  id: uuid (unique, required)
  customer_id: uuid (foreign key to Customer, required)
  email: email (unique, required)
  role: enum [admin, member, viewer] (required)
  last_login_at: timestamp
  is_active: boolean (default: true)

relationships:
  - belongs_to: Customer
  - has_many: UsageEvents
  - has_many: SupportTickets

data_quality_rules:
  - "One user cannot belong to multiple customers"
  - "Admin role requires email_verified = true"

governance:
  owner: "Product team"
  PII: true
  retention: "Delete within 30 days of account deletion (GDPR)"
```

#### 3. **UsageEvent** (Product telemetry)

```yaml
entity: UsageEvent
description: "Timestamped record of user action in product"
schema:
  id: uuid (unique, required)
  user_id: uuid (foreign key, required)
  event_name: string (required, e.g., "feature.sso.enabled")
  event_properties: json (optional, e.g., {"num_users": 50})
  timestamp: timestamp (required, indexed)

relationships:
  - belongs_to: User
  - belongs_to: Customer (via User)

data_quality_rules:
  - "event_name must follow naming convention: <domain>.<action>.<object>"
  - "timestamp cannot be in the future"
  - "event_properties must be valid JSON"

governance:
  owner: "Product & Analytics teams"
  PII: false (aggregated, anonymized)
  retention: "2 years (rolling window)"
```

#### 4. **SupportTicket** (Customer service)

```yaml
entity: SupportTicket
description: "Customer support request"
schema:
  id: uuid
  customer_id: uuid (foreign key)
  user_id: uuid (optional - who submitted)
  subject: string (required)
  status: enum [open, in_progress, resolved, closed]
  priority: enum [low, medium, high, critical]
  created_at: timestamp
  resolved_at: timestamp (nullable)

relationships:
  - belongs_to: Customer
  - belongs_to: User (optional)

data_quality_rules:
  - "Resolved tickets must have resolved_at timestamp"
  - "Critical priority tickets must be assigned within 1 hour"

governance:
  owner: "Customer Support team"
  PII: true (may contain customer names, emails in messages)
  retention: "5 years"
```

---

### Naming Conventions (Critical!)

**Why it matters:** Inconsistent naming = broken integrations, confused AI agents

**Rules:**

1. **Entity names:** Singular, PascalCase
   - ‚úÖ `Customer`, `User`, `SupportTicket`
   - ‚ùå `customers`, `support-tickets`

2. **Field names:** snake_case
   - ‚úÖ `created_at`, `annual_revenue`, `email_verified`
   - ‚ùå `createdAt`, `AnnualRevenue`, `emailverified`

3. **Event names:** dot.notation.lowercase
   - ‚úÖ `customer.created`, `feature.sso.enabled`, `invoice.paid`
   - ‚ùå `CustomerCreated`, `SSO_ENABLED`

4. **Enum values:** lowercase, underscore
   - ‚úÖ `active`, `in_progress`, `churned`
   - ‚ùå `Active`, `In Progress`, `CHURNED`

**Document it:** Create a data dictionary, share with all teams

---

## Part 3: Event-Driven Architecture

### From Batch to Real-Time

**Old Way (Batch):**
```
Salesforce updated at 2 PM
    ‚Üì
Nightly ETL job runs at midnight (10 hours later)
    ‚Üì
Data warehouse updated at 1 AM
    ‚Üì
Finance sees update at 9 AM next day (19 hours delay)
```

**Problems:**
- Stale data (decisions based on yesterday's information)
- High latency (can't react quickly)
- Batch job failures = data missing

---

**New Way (Event-Driven):**
```
Salesforce: Deal closed at 2:00:00 PM
    ‚Üì
Event published: "deal.closed" (2:00:01 PM)
    ‚Üì
Event subscribers receive notification (2:00:02 PM):
    ‚îú‚îÄ> Data warehouse updates (2:00:03 PM)
    ‚îú‚îÄ> Finance dashboard refreshes (2:00:05 PM)
    ‚îú‚îÄ> CS team notified (2:00:06 PM)
    ‚îî‚îÄ> Onboarding automation triggered (2:00:10 PM)
```

**Benefits:**
- Real-time data (seconds, not hours)
- Decoupled systems (Salesforce doesn't need to know who consumes events)
- Resilient (if one subscriber fails, others still work)

---

### Event Schema Standard

**Template:**
```json
{
  "event_id": "evt_1a2b3c4d",
  "event_name": "deal.closed",
  "event_version": "v1",
  "timestamp": "2025-11-04T14:00:00Z",
  "source": "salesforce",
  "actor": {
    "type": "user",
    "id": "user_987",
    "email": "sales@example.com"
  },
  "data": {
    "deal_id": "deal_456",
    "customer_id": "cust_123",
    "amount": 50000,
    "currency": "USD",
    "contract_term": "annual"
  },
  "metadata": {
    "environment": "production",
    "trace_id": "trace_xyz"
  }
}
```

**Key Fields:**
- `event_id`: Unique identifier (for deduplication)
- `event_name`: What happened (dot notation)
- `timestamp`: When it happened (UTC)
- `source`: Which system produced the event
- `actor`: Who/what triggered it
- `data`: Payload (entity-specific)
- `metadata`: Tracing, debugging info

---

### Event Catalog (Examples)

| Event Name | Trigger | Subscribers | Data Payload |
|-----------|---------|-------------|--------------|
| `customer.created` | New customer signs up | CS (onboarding), Finance (setup billing), Marketing (welcome campaign) | customer_id, name, email, plan |
| `deal.closed` | Sales closes deal | Finance (revenue), CS (onboarding), Product (usage tracking) | deal_id, customer_id, amount, contract_term |
| `feature.enabled` | User enables feature | Product (analytics), CS (check if trained), Marketing (upsell if premium) | user_id, feature_name, timestamp |
| `ticket.created` | Customer opens ticket | Support (assign agent), Product (log bug if recurring), CS (check health score) | ticket_id, customer_id, subject, priority |
| `invoice.paid` | Payment received | Finance (revenue recognition), CS (update health score), Sales (commission) | invoice_id, customer_id, amount |
| `usage.threshold_exceeded` | Customer exceeds plan limit | CS (upsell opportunity), Product (throttle or notify), Billing (overage charges) | customer_id, metric, threshold, current_value |

---

### Technology Stack (Event Infrastructure)

**Options:**

1. **Simple (Small teams, <1000 events/day):**
   - **Webhooks:** Zapier, Make.com, n8n
   - **Cost:** $20-100/mo
   - **Pros:** Easy, no infrastructure
   - **Cons:** Limited reliability, no replay

2. **Intermediate (Medium teams, 1K-100K events/day):**
   - **Kafka** (self-hosted) or **Confluent Cloud** (managed)
   - **Cost:** $200-2K/mo
   - **Pros:** Reliable, scalable, replay-able
   - **Cons:** Complex to manage

3. **Managed (Prefer simplicity):**
   - **AWS EventBridge**, **Google Pub/Sub**, **Azure Event Grid**
   - **Cost:** $0.001/event (pay-as-you-go)
   - **Pros:** Fully managed, scales automatically
   - **Cons:** Cloud vendor lock-in

4. **Purpose-Built:**
   - **Segment** (customer data), **RudderStack** (open-source alternative)
   - **Cost:** $100-5K/mo
   - **Pros:** Built for customer events, integrates with 300+ tools
   - **Cons:** Expensive at scale

**Recommendation:**
- **Start:** Webhooks (Zapier) for 1-2 critical events
- **Scale:** EventBridge or Segment once >10 events/day
- **Mature:** Kafka if >100K events/day or need custom processing

---

## Part 4: Data Quality and Observability

### The 6 Dimensions of Data Quality

| Dimension | Definition | Example Issue | Solution |
|-----------|-----------|---------------|----------|
| **Completeness** | All required fields populated | Customer missing email (20% incomplete) | Validation: Reject records missing required fields |
| **Accuracy** | Data reflects reality | Customer status = "active" but subscription expired | Reconciliation: Cross-check with billing system daily |
| **Consistency** | Same data, same format everywhere | Salesforce says "$50K ARR", finance says "$48K" | Single source of truth: Data warehouse canonical record |
| **Timeliness** | Data is up-to-date | Deal closed yesterday, CS still sees "in progress" | Event-driven: Update in real-time, not nightly batch |
| **Validity** | Data conforms to schema | Email = "notanemail", phone = "abc123" | Schema validation: Enforce at write-time |
| **Uniqueness** | No duplicate records | Same customer appears twice (different IDs) | Deduplication: Match on email/domain, merge records |

---

### Data Quality Metrics (Track These)

**Dashboard:**
```
DATA QUALITY SCORECARD (Weekly)

Entity: Customer
  Completeness: 94% (6% missing industry field)
  Accuracy: 98% (2% have expired subscriptions but status = "active")
  Consistency: 100% (Salesforce and Data Warehouse match)
  Timeliness: 99.8% (avg sync delay: 2 seconds)
  Validity: 97% (3% have invalid email formats)
  Uniqueness: 99% (1% duplicate records detected, auto-merged)
  
Overall Score: 97.8% (üü¢ Healthy)

üö® ALERTS:
  - Industry field missing for 120 customers (‚Üë from 80 last week)
    Action: Sales team to enrich data during next customer call
  
  - 45 customers status = "active" but subscription expired
    Action: Finance to reconcile, CS to reach out (retention risk)
```

**Thresholds:**
- üü¢ **>95%**: Healthy (AI agents can trust this data)
- üü° **90-95%**: Warning (monitor closely, fix root causes)
- üî¥ **<90%**: Critical (pause AI agents using this data, fix immediately)

---

### Data Observability Tools

**What to Monitor:**
- **Schema changes:** Did someone add/remove a field without warning?
- **Volume anomalies:** Why did customer signups drop 50% today?
- **Freshness:** When was data last updated (should be <5 min)?
- **Null rate:** Are required fields suddenly null (data pipeline broken)?

**Tools:**
- **Open Source:** Great Expectations, dbt tests
- **Commercial:** Monte Carlo, Datadog Data Observability, Bigeye
- **Built-In:** Snowflake, BigQuery have native data quality checks

**Example Alert:**
```
üö® DATA QUALITY ALERT

Dataset: customer_events
Issue: Volume drop detected
Current: 120 events/hour (avg: 500 events/hour)
Duration: Last 2 hours
Severity: High

Possible Causes:
  - Event pipeline down (check EventBridge logs)
  - Product outage (check uptime monitor)
  - Data source issue (check Segment connection)

Action: Investigate within 30 minutes
Owner: Data Engineering team
```

---

## Part 5: Governance and Access Control

### Data Classification (Who Can See What)

**Classification Levels:**

| Level | Definition | Examples | Access |
|-------|-----------|----------|--------|
| **Public** | Safe to share externally | Product pricing, blog posts | Everyone (including customers) |
| **Internal** | Safe within company | Roadmap, OKRs, non-sensitive metrics | All employees |
| **Confidential** | Sensitive business data | Revenue, customer contracts, pipeline | Need-to-know (Finance, Sales leadership) |
| **Restricted** | Highly sensitive, regulated | PII, health data, financial records | Strict access controls, audit logs |

**Tag every dataset:**
```yaml
dataset: customer_profiles
classification: Restricted (contains PII)
access_policy:
  - role: Sales (read: name, email, company, status)
  - role: CS (read: name, email, company, status, usage, support history)
  - role: Finance (read: name, company, billing, revenue)
  - role: Marketing (read: email, company, industry - anonymized ID only)
  - role: Data Science (read: anonymized aggregates only, no PII)

audit_log: true (log every access)
retention: 7 years after customer churn (compliance)
encryption: AES-256 at rest, TLS 1.3 in transit
```

---

### GDPR / CCPA Compliance (Data Privacy)

**Requirements:**

1. **Right to Access:** Customer can request "what data do you have on me?"
   - **Solution:** Build self-service portal (e.g., "Download my data" button)

2. **Right to Deletion:** Customer can request "delete all my data"
   - **Solution:** Automate deletion workflow (remove from all systems within 30 days)

3. **Right to Portability:** Customer can request data in machine-readable format
   - **Solution:** Export to JSON/CSV

4. **Consent Management:** Track what customer consented to
   - **Solution:** Store consent records (marketing emails: yes/no, analytics: yes/no)

5. **Data Minimization:** Only collect necessary data
   - **Solution:** Review every field: "Do we need this? What's the business purpose?"

**Example: Deletion Workflow**
```
Customer requests deletion
    ‚Üì
DeletionRequest-Agent triggers workflow:
    ‚îú‚îÄ> CRM (Salesforce): Anonymize customer record (replace name/email with "DELETED")
    ‚îú‚îÄ> Product DB: Delete user account, usage data
    ‚îú‚îÄ> Data Warehouse: Flag customer_id as deleted (retain aggregates, delete PII)
    ‚îú‚îÄ> Marketing (HubSpot): Unsubscribe and delete email
    ‚îî‚îÄ> Backups: Mark for purge in next backup rotation
    ‚Üì
Confirmation email sent: "Your data has been deleted within 30 days"
    ‚Üì
Audit log: "customer_id=123 deleted on 2025-11-04, requested by user, completed by DeletionAgent"
```

---

### Role-Based Access Control (RBAC)

**Principle of Least Privilege:** Users get minimum access needed for their job

**Example Policy:**

| Role | Customer Data | Financial Data | Product Usage | Support Tickets |
|------|--------------|----------------|---------------|-----------------|
| **Sales Rep** | Read/Write (own accounts) | Read (ARR only) | None | Read (own customers) |
| **CS Manager** | Read (all customers) | Read (ARR, churn risk) | Read (all usage) | Read/Write (all tickets) |
| **Finance Analyst** | Read (anonymized) | Read/Write (all financials) | None | None |
| **Data Scientist** | Read (anonymized aggregates) | Read (aggregates) | Read (aggregates) | Read (aggregates) |
| **CEO** | Read (all) | Read (all) | Read (all) | Read (all) |
| **AI Agent** | Depends on agent role (see below) | Depends | Depends | Depends |

**AI Agent Access:**
- **ChurnPredictor-Agent:** Read customer data (usage, support tickets, billing) - no write access
- **InvoiceProcessor-Agent:** Read invoices, write approval status - no customer PII access
- **LeadQualifier-Agent:** Read lead data, write enrichment (company size, industry) - no financial access

---

## Part 6: Implementation Patterns

### Pattern 1: Customer 360 (Unified Customer View)

**Use Case:** CS team needs complete customer picture (sales, product usage, support, billing)

**Architecture:**
```
DATA SOURCES (Systems of Record):
  - Salesforce (CRM): Customer profile, deals, contacts
  - Stripe (Billing): Subscriptions, invoices, payment history
  - Mixpanel (Product): Usage events, feature adoption
  - Zendesk (Support): Tickets, satisfaction scores

    ‚Üì (Real-time event streams)

DATA SPINE (Data Warehouse):
  - customer_profiles table (denormalized)
    Fields: id, name, email, status, arr, industry, health_score
  
  - customer_usage_summary (updated hourly)
    Fields: customer_id, daily_active_users, features_used, last_login
  
  - customer_support_summary (updated on ticket events)
    Fields: customer_id, open_tickets, avg_response_time, csat_score

    ‚Üì (API / Dashboard)

CONSUMERS:
  - CSM Dashboard: See customer health, usage, tickets at a glance
  - ChurnPredictor-Agent: Analyze usage + support + billing patterns
  - EmailCampaign-Agent: Segment customers (high usage ‚Üí upsell, low usage ‚Üí re-engage)
```

**Implementation (4 weeks):**

**Week 1:** Define Customer schema (data contract)
```sql
CREATE TABLE customer_profiles (
  id UUID PRIMARY KEY,
  name VARCHAR(200) NOT NULL,
  email VARCHAR(200) UNIQUE NOT NULL,
  status VARCHAR(50) NOT NULL, -- prospect, trial, active, churned
  arr DECIMAL(10,2),
  industry VARCHAR(100),
  health_score INT, -- 0-100, calculated by AI
  created_at TIMESTAMP NOT NULL,
  updated_at TIMESTAMP NOT NULL
);
```

**Week 2:** Set up event streams
- Install Segment or EventBridge
- Configure sources: Salesforce, Stripe, Mixpanel, Zendesk
- Test: "customer.created" event fires when new signup

**Week 3:** Build aggregation jobs
- Hourly job: Calculate usage summary (daily active users, features used)
- Real-time: Update support summary on ticket events
- Daily job: Calculate health score (usage + support + billing signals)

**Week 4:** Build dashboard + AI agent
- CSM Dashboard: Shows customer 360 view
- ChurnPredictor-Agent: Reads customer profiles, flags at-risk

---

### Pattern 2: Revenue Reporting (Finance Analytics)

**Use Case:** CFO needs real-time revenue visibility (ARR, MRR, churn, expansion)

**Architecture:**
```
DATA SOURCES:
  - Salesforce: Closed deals (ARR, contract term)
  - Stripe: Subscription events (new, upgraded, downgraded, churned)
  - Contracts DB: Custom pricing, discounts, renewals

    ‚Üì

DATA SPINE (Fact/Dimension Tables):
  - fact_revenue (daily snapshot)
    Fields: date, customer_id, mrr, arr, plan_type
  
  - dim_customers (customer attributes)
    Fields: customer_id, name, industry, signup_date
  
  - dim_plans (subscription tiers)
    Fields: plan_id, name, base_price

    ‚Üì

ANALYTICS:
  - Finance Dashboard: MRR trend, ARR by segment, churn rate
  - BudgetMonitor-Agent: Alerts if revenue trending below forecast
```

**SQL Example: Calculate MRR**
```sql
-- Monthly Recurring Revenue by customer
SELECT 
  customer_id,
  customer_name,
  plan_type,
  CASE 
    WHEN billing_frequency = 'monthly' THEN amount
    WHEN billing_frequency = 'annual' THEN amount / 12
    ELSE 0
  END as mrr,
  status
FROM subscriptions
WHERE status = 'active';

-- Total MRR
SELECT SUM(mrr) as total_mrr FROM (above query);

-- MRR Churn Rate (this month)
SELECT 
  (churned_mrr / beginning_mrr) * 100 as churn_rate
FROM (
  SELECT 
    SUM(CASE WHEN churned_at >= '2025-11-01' THEN mrr ELSE 0 END) as churned_mrr,
    SUM(CASE WHEN created_at < '2025-11-01' THEN mrr ELSE 0 END) as beginning_mrr
  FROM subscriptions
);
```

---

### Pattern 3: Product Analytics (Feature Adoption)

**Use Case:** Product team needs to know which features are used, by whom, how often

**Architecture:**
```
DATA SOURCE:
  - Product instrumentation: Log events (feature.enabled, feature.used, feature.disabled)

    ‚Üì

DATA SPINE (Event Stream):
  - usage_events table (append-only, billions of rows)
    Fields: event_id, user_id, customer_id, event_name, properties, timestamp

    ‚Üì (Aggregation)

  - feature_adoption_summary (daily rollup)
    Fields: feature_name, customers_using, users_using, usage_count, date

    ‚Üì

ANALYTICS:
  - Product Dashboard: Feature adoption curves, engagement metrics
  - FeatureAdoption-Agent: Flags underutilized features ‚Üí CS reaches out to train
```

**Example Query: Feature Adoption Rate**
```sql
-- How many customers enabled SSO?
SELECT 
  COUNT(DISTINCT customer_id) as customers_with_sso
FROM usage_events
WHERE event_name = 'feature.sso.enabled'
  AND timestamp >= '2025-11-01';

-- What % of eligible customers use SSO?
SELECT 
  (customers_with_sso * 1.0 / total_enterprise_customers) * 100 as adoption_rate
FROM (
  SELECT COUNT(*) as total_enterprise_customers
  FROM customers
  WHERE plan_type = 'enterprise' AND status = 'active'
);

-- SSO usage trend (daily)
SELECT 
  DATE(timestamp) as date,
  COUNT(DISTINCT customer_id) as daily_sso_usage
FROM usage_events
WHERE event_name = 'feature.sso.used'
GROUP BY DATE(timestamp)
ORDER BY date DESC;
```

---

## Part 7: Data Spine Maturity Model

### Level 1: **Ad Hoc** (Most Companies Start Here)

**Characteristics:**
- Data lives in isolated systems (Salesforce, QuickBooks, spreadsheets)
- Manual exports/imports (CSV hell)
- No data contracts, no shared vocabulary
- Decisions based on gut feel or outdated reports

**Pain Points:**
- Finance can't see Sales pipeline
- CS doesn't know who's churning
- AI agents can't function (no clean data)

**Time to Fix:** 2-4 weeks (build basic data spine)

---

### Level 2: **Centralized** (Single Source of Truth)

**Characteristics:**
- Data warehouse exists (Snowflake, BigQuery, Redshift)
- Nightly batch ETL (data updated once/day)
- Basic data contracts (schema documented)
- Reports auto-generated

**Pain Points:**
- Data stale (batch updates, not real-time)
- Limited data quality enforcement
- AI agents trained on yesterday's data

**Time to Achieve:** 2-3 months from Level 1

---

### Level 3: **Real-Time** (Event-Driven)

**Characteristics:**
- Event streams (EventBridge, Kafka, Segment)
- Real-time updates (seconds, not hours)
- Data quality monitoring (Great Expectations, Monte Carlo)
- AI agents consume events, act immediately

**Pain Points:**
- Complexity (managing event infrastructure)
- Cost (event streaming not free)

**Time to Achieve:** 4-6 months from Level 2

---

### Level 4: **AI-Native** (Self-Service, Self-Healing)

**Characteristics:**
- Data contracts enforced automatically (schema validation)
- Data quality self-healing (AI detects and fixes anomalies)
- Self-service analytics (business users query without SQL)
- Federated governance (squads own their data, central oversight)

**Pain Points:**
- Requires mature org (data engineers, governance processes)
- High investment ($100K-500K/year in tooling + team)

**Time to Achieve:** 12-18 months from Level 3

**Goal:** Most organizations should aim for Level 3 within 12 months

---

## Part 8: Quick Start Checklist

### Week 1-2: Foundation

- [ ] Identify core entities (Customer, User, Invoice, etc.)
- [ ] Define data contracts (schema, validation rules) for top 3 entities
- [ ] Document naming conventions (share with all teams)
- [ ] Choose data warehouse (Snowflake, BigQuery, Redshift)
- [ ] Set up basic ETL (Fivetran, Airbyte, or custom scripts)

### Week 3-4: Integration

- [ ] Connect top 3 data sources (CRM, billing, product analytics)
- [ ] Test data sync (does Salesforce customer appear in warehouse?)
- [ ] Build first dashboard (Customer 360 or Revenue Reporting)
- [ ] Deploy first AI agent (uses warehouse data, not direct API)

### Month 2: Quality & Governance

- [ ] Implement data quality monitoring (Great Expectations or similar)
- [ ] Set up alerts (data freshness, null rates, schema changes)
- [ ] Define access policies (RBAC - who can see what)
- [ ] Document data classification (Public, Internal, Confidential, Restricted)

### Month 3: Real-Time & Events

- [ ] Deploy event infrastructure (EventBridge or Segment)
- [ ] Publish first event (e.g., "customer.created")
- [ ] Build event-driven workflow (customer signup ‚Üí onboarding automation)
- [ ] Migrate batch ETL ‚Üí real-time events (for critical data)

### Month 4-6: Scale & Optimize

- [ ] Add 5-10 more data sources
- [ ] Deploy 3-5 AI agents (all using data spine)
- [ ] Implement self-service analytics (Looker, Tableau, Metabase)
- [ ] Quarterly governance review (data quality scorecard, access audit)

---

## Conclusion: Data is the Foundation

**Without a solid Data Spine:**
- ‚ùå AI agents fail (garbage in, garbage out)
- ‚ùå Automation breaks (inconsistent data formats)
- ‚ùå Decisions are guesses (no single source of truth)
- ‚ùå Teams work in silos (can't see each other's data)

**With a solid Data Spine:**
- ‚úÖ AI agents accurate (clean, consistent data)
- ‚úÖ Automation reliable (schema validated, event-driven)
- ‚úÖ Decisions data-driven (real-time dashboards)
- ‚úÖ Teams coordinated (shared visibility)

**Start simple, build incrementally:**
1. **Week 1:** Define 3 core entities
2. **Month 1:** Build basic warehouse
3. **Month 3:** Add real-time events
4. **Month 6:** AI-native data operations

The data spine isn't glamorous, but it's the difference between an AI-native organization and an AI-theater organization.

**Build the foundation first. Everything else follows.**

---

**Next Steps:**
- [Integration Guide](solid-ai-integration-guide.md) - How data spine connects to other layers
- [Implementing AI Agents](implementing-ai-agents-practical-guide.md) - Build agents that use your data
- [Architecture](../DOCS/02-architecture.md) - Deep dive on Layer 2 (Data Spine)

---

**Version:** 1.0  
**Last Updated:** November 2025  
**Framework:** SOLID.AI  
**License:** MIT
